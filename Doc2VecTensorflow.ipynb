{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Text Helper Functions\n",
        "#---------------------------------------\n",
        "#\n",
        "# We pull out text helper functions to reduce redundant code\n",
        "\n",
        "import string\n",
        "import os\n",
        "import urllib.request\n",
        "import io\n",
        "import tarfile\n",
        "import collections\n",
        "import numpy as np\n",
        "import requests\n",
        "import gzip\n",
        "class text_helpers:\n",
        "  # Normalize text\n",
        "  def normalize_text(texts, stops):\n",
        "      # Lower case\n",
        "      texts = [x.lower() for x in texts]\n",
        "\n",
        "      # Remove punctuation\n",
        "      texts = [''.join(c for c in x if c not in string.punctuation) for x in texts]\n",
        "\n",
        "      # Remove numbers\n",
        "      texts = [''.join(c for c in x if c not in '0123456789') for x in texts]\n",
        "\n",
        "      # Remove stopwords\n",
        "      texts = [' '.join([word for word in x.split() if word not in (stops)]) for x in texts]\n",
        "\n",
        "      # Trim extra whitespace\n",
        "      texts = [' '.join(x.split()) for x in texts]\n",
        "\n",
        "      return(texts)\n",
        "\n",
        "\n",
        "  # Build dictionary of words\n",
        "  def build_dictionary(sentences, vocabulary_size):\n",
        "      # Turn sentences (list of strings) into lists of words\n",
        "      split_sentences = [s.split() for s in sentences]\n",
        "      words = [x for sublist in split_sentences for x in sublist]\n",
        "\n",
        "      # Initialize list of [word, word_count] for each word, starting with unknown\n",
        "      count = [['RARE', -1]]\n",
        "\n",
        "      # Now add most frequent words, limited to the N-most frequent (N=vocabulary size)\n",
        "      count.extend(collections.Counter(words).most_common(vocabulary_size-1))\n",
        "\n",
        "      # Now create the dictionary\n",
        "      word_dict = {}\n",
        "      # For each word, that we want in the dictionary, add it, then make it\n",
        "      # the value of the prior dictionary length\n",
        "      for word, word_count in count:\n",
        "          word_dict[word] = len(word_dict)\n",
        "\n",
        "      return(word_dict)\n",
        "\n",
        "\n",
        "  # Turn text data into lists of integers from dictionary\n",
        "  def text_to_numbers(sentences, word_dict):\n",
        "      # Initialize the returned data\n",
        "      data = []\n",
        "      for sentence in sentences:\n",
        "          sentence_data = []\n",
        "          # For each word, either use selected index or rare word index\n",
        "          for word in sentence.split():\n",
        "              if word in word_dict:\n",
        "                  word_ix = word_dict[word]\n",
        "              else:\n",
        "                  word_ix = 0\n",
        "              sentence_data.append(word_ix)\n",
        "          data.append(sentence_data)\n",
        "      return(data)\n",
        "\n",
        "\n",
        "  # Generate data randomly (N words behind, target, N words ahead)\n",
        "  def generate_batch_data(sentences, batch_size, window_size, method='skip_gram'):\n",
        "      # Fill up data batch\n",
        "      batch_data = []\n",
        "      label_data = []\n",
        "      while len(batch_data) < batch_size:\n",
        "          # select random sentence to start\n",
        "          rand_sentence_ix = int(np.random.choice(len(sentences), size=1))\n",
        "          rand_sentence = sentences[rand_sentence_ix]\n",
        "          # Generate consecutive windows to look at\n",
        "          window_sequences = [rand_sentence[max((ix-window_size),0):(ix+window_size+1)] for ix, x in enumerate(rand_sentence)]\n",
        "          # Denote which element of each window is the center word of interest\n",
        "          label_indices = [ix if ix<window_size else window_size for ix,x in enumerate(window_sequences)]\n",
        "\n",
        "          # Pull out center word of interest for each window and create a tuple for each window\n",
        "          if method=='skip_gram':\n",
        "              batch_and_labels = [(x[y], x[:y] + x[(y+1):]) for x,y in zip(window_sequences, label_indices)]\n",
        "              # Make it in to a big list of tuples (target word, surrounding word)\n",
        "              tuple_data = [(x, y_) for x,y in batch_and_labels for y_ in y]\n",
        "              batch, labels = [list(x) for x in zip(*tuple_data)]\n",
        "          elif method=='cbow':\n",
        "              batch_and_labels = [(x[:y] + x[(y+1):], x[y]) for x,y in zip(window_sequences, label_indices)]\n",
        "              # Only keep windows with consistent 2*window_size\n",
        "              batch_and_labels = [(x,y) for x,y in batch_and_labels if len(x)==2*window_size]\n",
        "              batch, labels = [list(x) for x in zip(*batch_and_labels)]\n",
        "          elif method=='doc2vec':\n",
        "              # For doc2vec we keep LHS window only to predict target word\n",
        "              batch_and_labels = [(rand_sentence[i:i+window_size], rand_sentence[i+window_size]) for i in range(0, len(rand_sentence)-window_size)]\n",
        "              batch, labels = [list(x) for x in zip(*batch_and_labels)]\n",
        "              # Add document index to batch!! Remember that we must extract the last index in batch for the doc-index\n",
        "              batch = [x + [rand_sentence_ix] for x in batch]\n",
        "          else:\n",
        "              raise ValueError('Method {} not implemented yet.'.format(method))\n",
        "\n",
        "          # extract batch and labels\n",
        "          batch_data.extend(batch[:batch_size])\n",
        "          label_data.extend(labels[:batch_size])\n",
        "      # Trim batch and label at the end\n",
        "      batch_data = batch_data[:batch_size]\n",
        "      label_data = label_data[:batch_size]\n",
        "\n",
        "      # Convert to numpy array\n",
        "      batch_data = np.array(batch_data)\n",
        "      label_data = np.transpose(np.array([label_data]))\n",
        "\n",
        "      return(batch_data, label_data)\n",
        "\n",
        "\n",
        "  # Load the movie review data\n",
        "  # Check if data was downloaded, otherwise download it and save for future use\n",
        "  def load_movie_data():\n",
        "      save_folder_name = 'temp'\n",
        "      pos_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.pos')\n",
        "      neg_file = os.path.join(save_folder_name, 'rt-polaritydata', 'rt-polarity.neg')\n",
        "\n",
        "      # Check if files are already downloaded\n",
        "      if not os.path.exists(os.path.join(save_folder_name, 'rt-polaritydata')):\n",
        "          movie_data_url = 'http://www.cs.cornell.edu/people/pabo/movie-review-data/rt-polaritydata.tar.gz'\n",
        "\n",
        "          # Save tar.gz file\n",
        "          req = requests.get(movie_data_url, stream=True)\n",
        "          with open('temp_movie_review_temp.tar.gz', 'wb') as f:\n",
        "              for chunk in req.iter_content(chunk_size=1024):\n",
        "                  if chunk:\n",
        "                      f.write(chunk)\n",
        "                      f.flush()\n",
        "          # Extract tar.gz file into temp folder\n",
        "          tar = tarfile.open('temp_movie_review_temp.tar.gz', \"r:gz\")\n",
        "          tar.extractall(path='temp')\n",
        "          tar.close()\n",
        "\n",
        "      pos_data = []\n",
        "      with open(pos_file, 'r', encoding='latin-1') as f:\n",
        "          for line in f:\n",
        "              pos_data.append(line.encode('ascii',errors='ignore').decode())\n",
        "      f.close()\n",
        "      pos_data = [x.rstrip() for x in pos_data]\n",
        "\n",
        "      neg_data = []\n",
        "      with open(neg_file, 'r', encoding='latin-1') as f:\n",
        "          for line in f:\n",
        "              neg_data.append(line.encode('ascii',errors='ignore').decode())\n",
        "      f.close()\n",
        "      neg_data = [x.rstrip() for x in neg_data]\n",
        "\n",
        "      texts = pos_data + neg_data\n",
        "      target = [1]*len(pos_data) + [0]*len(neg_data)\n",
        "\n",
        "      return(texts, target)"
      ],
      "metadata": {
        "id": "gEK4HjrwTbwo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load movie review data\n",
        "texts, targets = text_helpers.load_movie_data()\n",
        "\n",
        "# Display a few reviews and their labels\n",
        "for i in range(5):  # Print the first 5 reviews\n",
        "    print(f\"Review {i + 1}:\")\n",
        "    print(texts[i])\n",
        "    print(f\"Label: {'Positive' if targets[i] == 1 else 'Negative'}\")\n",
        "    print(\"=\" * 50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d00h3fRhlpal",
        "outputId": "7e49f40e-007e-458c-b597-0ea415df15fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Review 1:\n",
            "the rock is destined to be the 21st century's new \" conan \" and that he's going to make a splash even greater than arnold schwarzenegger , jean-claud van damme or steven segal .\n",
            "Label: Positive\n",
            "==================================================\n",
            "Review 2:\n",
            "the gorgeously elaborate continuation of \" the lord of the rings \" trilogy is so huge that a column of words cannot adequately describe co-writer/director peter jackson's expanded vision of j . r . r . tolkien's middle-earth .\n",
            "Label: Positive\n",
            "==================================================\n",
            "Review 3:\n",
            "effective but too-tepid biopic\n",
            "Label: Positive\n",
            "==================================================\n",
            "Review 4:\n",
            "if you sometimes like to go to the movies to have fun , wasabi is a good place to start .\n",
            "Label: Positive\n",
            "==================================================\n",
            "Review 5:\n",
            "emerges as something rare , an issue movie that's so honest and keenly observed that it doesn't feel like one .\n",
            "Label: Positive\n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import random\n",
        "import os\n",
        "import pickle\n",
        "import string\n",
        "import requests\n",
        "import collections\n",
        "import io\n",
        "import tarfile\n",
        "import urllib.request\n",
        "from nltk.corpus import stopwords\n",
        "from tensorflow.python.framework import ops\n",
        "\n",
        "# Uncomment the following line if you want to install NLTK in Colab\n",
        "# !pip install nltk\n",
        "\n",
        "ops.reset_default_graph()\n",
        "\n",
        "# Declare model parameters\n",
        "batch_size = 500\n",
        "vocabulary_size = 7500\n",
        "generations = 100000\n",
        "model_learning_rate = 0.001\n",
        "\n",
        "embedding_size = 200   # Word embedding size\n",
        "doc_embedding_size = 100   # Document embedding size\n",
        "concatenated_size = embedding_size + doc_embedding_size\n",
        "\n",
        "num_sampled = int(batch_size/2)    # Number of negative examples to sample.\n",
        "window_size = 3       # How many words to consider to the left.\n",
        "\n",
        "# Add checkpoints to training\n",
        "save_embeddings_every = 5000\n",
        "print_valid_every = 5000\n",
        "print_loss_every = 100\n",
        "\n",
        "# Declare stop words\n",
        "# stops = stopwords.words('english')\n",
        "stops = []\n",
        "\n",
        "# We pick a few test words for validation.\n",
        "valid_words = ['love', 'hate', 'happy', 'sad', 'man', 'woman']\n",
        "# Later we will have to transform these into indices\n",
        "\n",
        "# Load the movie review data\n",
        "print('Loading Data')\n",
        "texts, target = text_helpers.load_movie_data()\n",
        "\n",
        "# Normalize text\n",
        "print('Normalizing Text Data')\n",
        "texts = text_helpers.normalize_text(texts, stops)\n",
        "\n",
        "# Texts must contain at least 3 words\n",
        "target = [target[ix] for ix, x in enumerate(texts) if len(x.split()) > window_size]\n",
        "texts = [x for x in texts if len(x.split()) > window_size]\n",
        "assert(len(target)==len(texts))\n",
        "\n",
        "# Build our data set and dictionaries\n",
        "print('Creating Dictionary')\n",
        "word_dictionary = text_helpers.build_dictionary(texts, vocabulary_size)\n",
        "word_dictionary_rev = dict(zip(word_dictionary.values(), word_dictionary.keys()))\n",
        "text_data = text_helpers.text_to_numbers(texts, word_dictionary)\n",
        "\n",
        "# Get validation word keys\n",
        "valid_examples = [word_dictionary[x] for x in valid_words]\n",
        "\n",
        "print('Creating Model')\n",
        "# Define Embeddings:\n",
        "embeddings = tf.Variable(tf.random.uniform([vocabulary_size, embedding_size], -1.0, 1.0))\n",
        "doc_embeddings = tf.Variable(tf.random.uniform([len(texts), doc_embedding_size], -1.0, 1.0))\n",
        "\n",
        "# NCE loss parameters\n",
        "nce_weights = tf.Variable(tf.random.truncated_normal([vocabulary_size, concatenated_size],\n",
        "                                               stddev=1.0 / np.sqrt(concatenated_size)))\n",
        "nce_biases = tf.Variable(tf.zeros([vocabulary_size]))\n",
        "\n",
        "# Create data/target placeholders\n",
        "x_inputs = tf.Variable(tf.zeros([batch_size, window_size + 1], dtype=tf.int32))\n",
        "y_target = tf.Variable(tf.zeros([batch_size, 1], dtype=tf.int32))\n",
        "valid_dataset = tf.constant(valid_examples, dtype=tf.int32)\n",
        "\n",
        "# Lookup the word embedding\n",
        "# Add together element embeddings in window:\n",
        "embed = tf.zeros([batch_size, embedding_size])\n",
        "for element in range(window_size):\n",
        "    embed += tf.nn.embedding_lookup(embeddings, x_inputs[:, element])\n",
        "\n",
        "doc_indices = tf.slice(x_inputs, [0, window_size], [batch_size, 1])\n",
        "doc_embed = tf.nn.embedding_lookup(doc_embeddings, doc_indices)\n",
        "\n",
        "# concatenate embeddings\n",
        "final_embed = tf.concat(axis=1, values=[embed, tf.squeeze(doc_embed)])\n",
        "\n",
        "# Get loss from prediction\n",
        "loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
        "                                     biases=nce_biases,\n",
        "                                     labels=y_target,\n",
        "                                     inputs=final_embed,\n",
        "                                     num_sampled=num_sampled,\n",
        "                                     num_classes=vocabulary_size))\n",
        "\n",
        "# Create optimizer\n",
        "optimizer = tf.optimizers.SGD(learning_rate=model_learning_rate)\n",
        "\n",
        "# Create a function to perform one optimization step\n",
        "@tf.function\n",
        "def train_step(batch_inputs, batch_labels):\n",
        "    with tf.GradientTape() as tape:\n",
        "        # Run the forward pass\n",
        "        embed = tf.zeros([batch_size, embedding_size])\n",
        "        for element in range(window_size):\n",
        "            embed += tf.nn.embedding_lookup(embeddings, batch_inputs[:, element])\n",
        "\n",
        "        doc_indices = tf.slice(batch_inputs, [0, window_size], [batch_size, 1])\n",
        "        doc_embed = tf.nn.embedding_lookup(doc_embeddings, doc_indices)\n",
        "        final_embed = tf.concat(axis=1, values=[embed, tf.squeeze(doc_embed)])\n",
        "\n",
        "        # Compute the loss\n",
        "        current_loss = tf.reduce_mean(tf.nn.nce_loss(weights=nce_weights,\n",
        "                                                     biases=nce_biases,\n",
        "                                                     labels=batch_labels,\n",
        "                                                     inputs=final_embed,\n",
        "                                                     num_sampled=num_sampled,\n",
        "                                                     num_classes=vocabulary_size))\n",
        "\n",
        "    # Compute gradients\n",
        "    grads = tape.gradient(current_loss, [embeddings, doc_embeddings, nce_weights, nce_biases])\n",
        "\n",
        "    # Apply gradients\n",
        "    optimizer.apply_gradients(zip(grads, [embeddings, doc_embeddings, nce_weights, nce_biases]))\n",
        "\n",
        "    return current_loss\n",
        "\n",
        "# Cosine similarity between words\n",
        "norm = tf.sqrt(tf.reduce_sum(tf.square(embeddings), 1, keepdims=True))\n",
        "normalized_embeddings = embeddings / norm\n",
        "valid_embeddings = tf.nn.embedding_lookup(normalized_embeddings, valid_dataset)\n",
        "similarity = tf.matmul(valid_embeddings, normalized_embeddings, transpose_b=True)\n",
        "\n",
        "# Create model saving operation\n",
        "saver = tf.compat.v1.train.Saver({\"embeddings\": embeddings, \"doc_embeddings\": doc_embeddings})\n",
        "\n",
        "# Add variable initializer.\n",
        "init = tf.compat.v1.global_variables_initializer()\n",
        "\n",
        "# The next line is not necessary in Colab, as it automatically starts a session.\n",
        "# sess.run(init)\n",
        "\n",
        "# Run the doc2vec model.\n",
        "print('Starting Training')\n",
        "loss_vec = []\n",
        "loss_x_vec = []\n",
        "for i in range(generations):\n",
        "    batch_inputs, batch_labels = text_helpers.generate_batch_data(text_data, batch_size,\n",
        "                                                                  window_size, method='doc2vec')\n",
        "    current_loss = train_step(batch_inputs, batch_labels)\n",
        "\n",
        "    # Return the loss\n",
        "    if (i+1) % print_loss_every == 0:\n",
        "        loss_vec.append(current_loss)\n",
        "        loss_x_vec.append(i+1)\n",
        "        print('Loss at step {} : {}'.format(i+1, current_loss))\n",
        "\n",
        "    # Validation: Print some random words and top 5 related words\n",
        "    if (i+1) % print_valid_every == 0:\n",
        "        sim = similarity.numpy()\n",
        "        for j in range(len(valid_words)):\n",
        "            valid_word = word_dictionary_rev[valid_examples[j]]\n",
        "            top_k = 5  # number of nearest neighbors\n",
        "            nearest = (-sim[j, :]).argsort()[1:top_k+1]\n",
        "            log_str = \"Nearest to {}:\".format(valid_word)\n",
        "            for k in range(top_k):\n",
        "                close_word = word_dictionary_rev[nearest[k]]\n",
        "                log_str = '{} {},'.format(log_str, close_word)\n",
        "            print(log_str)\n",
        "\n",
        "    # Save dictionary + embeddings\n",
        "    if (i+1) % save_embeddings_every == 0:\n",
        "        # Save vocabulary dictionary\n",
        "        with open(os.path.join(data_folder_name, 'movie_vocab.pkl'), 'wb') as f:\n",
        "            pickle.dump(word_dictionary, f)\n",
        "\n",
        "        # Save embeddings\n",
        "        model_checkpoint_path = os.path.join(os.getcwd(), data_folder_name, 'doc2vec_movie_embeddings.ckpt')\n",
        "        save_path = saver.save(tf.compat.v1.Session(), model_checkpoint_path)\n",
        "        print('Model saved in file: {}'.format(save_path))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1XRnX749XVj",
        "outputId": "031e1047-6792-42b2-b6b7-2cb8ca92dc8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading Data\n",
            "Normalizing Text Data\n",
            "Creating Dictionary\n",
            "Creating Model\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Saver is deprecated, please switch to tf.train.Checkpoint or tf.keras.Model.save_weights for training checkpoints. When executing eagerly variables do not necessarily have unique names, and so the variable.name-based lookups Saver performs are error-prone.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting Training\n",
            "Loss at step 100 : 627.3471069335938\n",
            "Loss at step 200 : 602.0684204101562\n",
            "Loss at step 300 : 591.26904296875\n",
            "Loss at step 400 : 566.1998901367188\n",
            "Loss at step 500 : 568.5272827148438\n",
            "Loss at step 600 : 530.0843505859375\n",
            "Loss at step 700 : 532.084228515625\n",
            "Loss at step 800 : 499.2488708496094\n",
            "Loss at step 900 : 502.57891845703125\n",
            "Loss at step 1000 : 495.83477783203125\n",
            "Loss at step 1100 : 492.978271484375\n",
            "Loss at step 1200 : 459.86895751953125\n",
            "Loss at step 1300 : 466.2934265136719\n",
            "Loss at step 1400 : 438.1857604980469\n",
            "Loss at step 1500 : 441.8061828613281\n",
            "Loss at step 1600 : 434.389892578125\n",
            "Loss at step 1700 : 412.6051025390625\n",
            "Loss at step 1800 : 342.7693786621094\n",
            "Loss at step 1900 : 405.4205322265625\n",
            "Loss at step 2000 : 358.5806884765625\n",
            "Loss at step 2100 : 333.2110595703125\n",
            "Loss at step 2200 : 364.3914794921875\n",
            "Loss at step 2300 : 346.8654479980469\n",
            "Loss at step 2400 : 335.8286437988281\n",
            "Loss at step 2500 : 306.0010681152344\n",
            "Loss at step 2600 : 319.6458740234375\n",
            "Loss at step 2700 : 260.3614807128906\n",
            "Loss at step 2800 : 279.81146240234375\n",
            "Loss at step 2900 : 306.7508850097656\n",
            "Loss at step 3000 : 310.8719787597656\n",
            "Loss at step 3100 : 234.9980926513672\n",
            "Loss at step 3200 : 263.5079345703125\n",
            "Loss at step 3300 : 261.1107482910156\n",
            "Loss at step 3400 : 250.9188995361328\n",
            "Loss at step 3500 : 286.74005126953125\n",
            "Loss at step 3600 : 262.4083251953125\n",
            "Loss at step 3700 : 245.2126007080078\n",
            "Loss at step 3800 : 246.47203063964844\n",
            "Loss at step 3900 : 205.33193969726562\n",
            "Loss at step 4000 : 172.5620574951172\n",
            "Loss at step 4100 : 188.32699584960938\n",
            "Loss at step 4200 : 212.7809600830078\n",
            "Loss at step 4300 : 167.37677001953125\n",
            "Loss at step 4400 : 203.5584716796875\n",
            "Loss at step 4500 : 166.5084686279297\n",
            "Loss at step 4600 : 189.4222869873047\n",
            "Loss at step 4700 : 189.82200622558594\n",
            "Loss at step 4800 : 145.1163787841797\n",
            "Loss at step 4900 : 126.48847198486328\n",
            "Loss at step 5000 : 164.77462768554688\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 5100 : 171.8027801513672\n",
            "Loss at step 5200 : 159.1332244873047\n",
            "Loss at step 5300 : 160.03639221191406\n",
            "Loss at step 5400 : 144.03237915039062\n",
            "Loss at step 5500 : 142.23924255371094\n",
            "Loss at step 5600 : 136.0523681640625\n",
            "Loss at step 5700 : 152.131591796875\n",
            "Loss at step 5800 : 117.30264282226562\n",
            "Loss at step 5900 : 157.00047302246094\n",
            "Loss at step 6000 : 132.0720977783203\n",
            "Loss at step 6100 : 140.43836975097656\n",
            "Loss at step 6200 : 169.08876037597656\n",
            "Loss at step 6300 : 125.99684143066406\n",
            "Loss at step 6400 : 114.29170227050781\n",
            "Loss at step 6500 : 117.35408782958984\n",
            "Loss at step 6600 : 120.96826934814453\n",
            "Loss at step 6700 : 136.3980712890625\n",
            "Loss at step 6800 : 123.2728271484375\n",
            "Loss at step 6900 : 119.41000366210938\n",
            "Loss at step 7000 : 131.2752685546875\n",
            "Loss at step 7100 : 114.76854705810547\n",
            "Loss at step 7200 : 102.4056396484375\n",
            "Loss at step 7300 : 115.90653228759766\n",
            "Loss at step 7400 : 116.06288146972656\n",
            "Loss at step 7500 : 115.04366302490234\n",
            "Loss at step 7600 : 116.27513885498047\n",
            "Loss at step 7700 : 115.31426239013672\n",
            "Loss at step 7800 : 89.29863739013672\n",
            "Loss at step 7900 : 130.6868133544922\n",
            "Loss at step 8000 : 87.20552825927734\n",
            "Loss at step 8100 : 106.90263366699219\n",
            "Loss at step 8200 : 101.80712127685547\n",
            "Loss at step 8300 : 153.88380432128906\n",
            "Loss at step 8400 : 106.59835052490234\n",
            "Loss at step 8500 : 88.58393859863281\n",
            "Loss at step 8600 : 123.42642974853516\n",
            "Loss at step 8700 : 83.69876861572266\n",
            "Loss at step 8800 : 94.26094818115234\n",
            "Loss at step 8900 : 92.63847351074219\n",
            "Loss at step 9000 : 115.66946411132812\n",
            "Loss at step 9100 : 82.07293701171875\n",
            "Loss at step 9200 : 107.7389907836914\n",
            "Loss at step 9300 : 114.85577392578125\n",
            "Loss at step 9400 : 102.5708236694336\n",
            "Loss at step 9500 : 92.4228515625\n",
            "Loss at step 9600 : 83.09945678710938\n",
            "Loss at step 9700 : 82.31379699707031\n",
            "Loss at step 9800 : 76.46733093261719\n",
            "Loss at step 9900 : 98.33113861083984\n",
            "Loss at step 10000 : 81.5210952758789\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 10100 : 86.39151763916016\n",
            "Loss at step 10200 : 75.65323638916016\n",
            "Loss at step 10300 : 75.07347869873047\n",
            "Loss at step 10400 : 71.48600769042969\n",
            "Loss at step 10500 : 76.70166778564453\n",
            "Loss at step 10600 : 84.95538330078125\n",
            "Loss at step 10700 : 97.53994750976562\n",
            "Loss at step 10800 : 80.73313903808594\n",
            "Loss at step 10900 : 78.67875671386719\n",
            "Loss at step 11000 : 80.2155532836914\n",
            "Loss at step 11100 : 83.40223693847656\n",
            "Loss at step 11200 : 65.52116394042969\n",
            "Loss at step 11300 : 73.55149841308594\n",
            "Loss at step 11400 : 74.42357635498047\n",
            "Loss at step 11500 : 73.75003051757812\n",
            "Loss at step 11600 : 90.0225601196289\n",
            "Loss at step 11700 : 72.29979705810547\n",
            "Loss at step 11800 : 84.55115509033203\n",
            "Loss at step 11900 : 97.29937744140625\n",
            "Loss at step 12000 : 68.77949523925781\n",
            "Loss at step 12100 : 72.22608947753906\n",
            "Loss at step 12200 : 83.9457778930664\n",
            "Loss at step 12300 : 76.34915161132812\n",
            "Loss at step 12400 : 76.27830505371094\n",
            "Loss at step 12500 : 78.8828125\n",
            "Loss at step 12600 : 60.85712814331055\n",
            "Loss at step 12700 : 72.85408020019531\n",
            "Loss at step 12800 : 76.5765380859375\n",
            "Loss at step 12900 : 60.79346466064453\n",
            "Loss at step 13000 : 70.98703002929688\n",
            "Loss at step 13100 : 74.48870849609375\n",
            "Loss at step 13200 : 67.1095199584961\n",
            "Loss at step 13300 : 73.81010437011719\n",
            "Loss at step 13400 : 65.84618377685547\n",
            "Loss at step 13500 : 58.9055290222168\n",
            "Loss at step 13600 : 69.38333892822266\n",
            "Loss at step 13700 : 68.16424560546875\n",
            "Loss at step 13800 : 67.27071380615234\n",
            "Loss at step 13900 : 54.970603942871094\n",
            "Loss at step 14000 : 85.00948333740234\n",
            "Loss at step 14100 : 68.75872039794922\n",
            "Loss at step 14200 : 67.70698547363281\n",
            "Loss at step 14300 : 66.46483612060547\n",
            "Loss at step 14400 : 53.663089752197266\n",
            "Loss at step 14500 : 58.39006805419922\n",
            "Loss at step 14600 : 57.89946365356445\n",
            "Loss at step 14700 : 51.27246856689453\n",
            "Loss at step 14800 : 67.13143157958984\n",
            "Loss at step 14900 : 81.12948608398438\n",
            "Loss at step 15000 : 60.24519348144531\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 15100 : 62.77875900268555\n",
            "Loss at step 15200 : 42.45981216430664\n",
            "Loss at step 15300 : 59.723793029785156\n",
            "Loss at step 15400 : 66.17481994628906\n",
            "Loss at step 15500 : 77.5613021850586\n",
            "Loss at step 15600 : 48.59528350830078\n",
            "Loss at step 15700 : 67.486083984375\n",
            "Loss at step 15800 : 62.9932746887207\n",
            "Loss at step 15900 : 58.6658935546875\n",
            "Loss at step 16000 : 51.7806510925293\n",
            "Loss at step 16100 : 62.77647018432617\n",
            "Loss at step 16200 : 69.33126068115234\n",
            "Loss at step 16300 : 58.232765197753906\n",
            "Loss at step 16400 : 55.56411361694336\n",
            "Loss at step 16500 : 56.825313568115234\n",
            "Loss at step 16600 : 55.12441635131836\n",
            "Loss at step 16700 : 65.71312713623047\n",
            "Loss at step 16800 : 55.39878845214844\n",
            "Loss at step 16900 : 52.20526885986328\n",
            "Loss at step 17000 : 64.9390869140625\n",
            "Loss at step 17100 : 52.70348358154297\n",
            "Loss at step 17200 : 49.20597839355469\n",
            "Loss at step 17300 : 52.57461166381836\n",
            "Loss at step 17400 : 74.65331268310547\n",
            "Loss at step 17500 : 56.58983612060547\n",
            "Loss at step 17600 : 59.10393142700195\n",
            "Loss at step 17700 : 39.80530548095703\n",
            "Loss at step 17800 : 51.42721176147461\n",
            "Loss at step 17900 : 59.34162521362305\n",
            "Loss at step 18000 : 57.1961555480957\n",
            "Loss at step 18100 : 38.71469497680664\n",
            "Loss at step 18200 : 46.31045150756836\n",
            "Loss at step 18300 : 54.81688690185547\n",
            "Loss at step 18400 : 40.42020797729492\n",
            "Loss at step 18500 : 55.98207092285156\n",
            "Loss at step 18600 : 58.53896713256836\n",
            "Loss at step 18700 : 53.673919677734375\n",
            "Loss at step 18800 : 50.22154998779297\n",
            "Loss at step 18900 : 61.08955383300781\n",
            "Loss at step 19000 : 54.49102020263672\n",
            "Loss at step 19100 : 61.05760955810547\n",
            "Loss at step 19200 : 48.583621978759766\n",
            "Loss at step 19300 : 51.81007766723633\n",
            "Loss at step 19400 : 37.0246696472168\n",
            "Loss at step 19500 : 47.1450080871582\n",
            "Loss at step 19600 : 42.00227355957031\n",
            "Loss at step 19700 : 57.923805236816406\n",
            "Loss at step 19800 : 39.791343688964844\n",
            "Loss at step 19900 : 43.83879852294922\n",
            "Loss at step 20000 : 54.39206314086914\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 20100 : 42.810672760009766\n",
            "Loss at step 20200 : 63.77251434326172\n",
            "Loss at step 20300 : 56.19245147705078\n",
            "Loss at step 20400 : 43.43321990966797\n",
            "Loss at step 20500 : 41.980587005615234\n",
            "Loss at step 20600 : 50.60085678100586\n",
            "Loss at step 20700 : 56.412200927734375\n",
            "Loss at step 20800 : 49.69403839111328\n",
            "Loss at step 20900 : 48.39461898803711\n",
            "Loss at step 21000 : 43.494476318359375\n",
            "Loss at step 21100 : 48.26932144165039\n",
            "Loss at step 21200 : 46.91843795776367\n",
            "Loss at step 21300 : 48.087738037109375\n",
            "Loss at step 21400 : 42.5004768371582\n",
            "Loss at step 21500 : 43.17951202392578\n",
            "Loss at step 21600 : 46.88459396362305\n",
            "Loss at step 21700 : 39.159297943115234\n",
            "Loss at step 21800 : 44.75939178466797\n",
            "Loss at step 21900 : 54.22283935546875\n",
            "Loss at step 22000 : 52.76463317871094\n",
            "Loss at step 22100 : 42.1132926940918\n",
            "Loss at step 22200 : 44.443199157714844\n",
            "Loss at step 22300 : 40.29596710205078\n",
            "Loss at step 22400 : 39.878143310546875\n",
            "Loss at step 22500 : 54.39218521118164\n",
            "Loss at step 22600 : 52.53689193725586\n",
            "Loss at step 22700 : 44.971954345703125\n",
            "Loss at step 22800 : 69.87149047851562\n",
            "Loss at step 22900 : 53.14364242553711\n",
            "Loss at step 23000 : 45.75139617919922\n",
            "Loss at step 23100 : 54.37335968017578\n",
            "Loss at step 23200 : 54.452701568603516\n",
            "Loss at step 23300 : 53.54173278808594\n",
            "Loss at step 23400 : 44.08039855957031\n",
            "Loss at step 23500 : 42.582862854003906\n",
            "Loss at step 23600 : 49.64554977416992\n",
            "Loss at step 23700 : 32.18427658081055\n",
            "Loss at step 23800 : 37.614383697509766\n",
            "Loss at step 23900 : 37.42275619506836\n",
            "Loss at step 24000 : 48.32482147216797\n",
            "Loss at step 24100 : 37.70242691040039\n",
            "Loss at step 24200 : 41.510555267333984\n",
            "Loss at step 24300 : 38.22637939453125\n",
            "Loss at step 24400 : 44.90979766845703\n",
            "Loss at step 24500 : 43.85976028442383\n",
            "Loss at step 24600 : 47.64226531982422\n",
            "Loss at step 24700 : 50.13194274902344\n",
            "Loss at step 24800 : 37.93488693237305\n",
            "Loss at step 24900 : 42.28205490112305\n",
            "Loss at step 25000 : 43.29798126220703\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 25100 : 40.63776397705078\n",
            "Loss at step 25200 : 46.15583419799805\n",
            "Loss at step 25300 : 44.44614028930664\n",
            "Loss at step 25400 : 48.983726501464844\n",
            "Loss at step 25500 : 35.262454986572266\n",
            "Loss at step 25600 : 32.82368469238281\n",
            "Loss at step 25700 : 38.548465728759766\n",
            "Loss at step 25800 : 30.727371215820312\n",
            "Loss at step 25900 : 32.02401351928711\n",
            "Loss at step 26000 : 43.69724655151367\n",
            "Loss at step 26100 : 49.90391540527344\n",
            "Loss at step 26200 : 38.484596252441406\n",
            "Loss at step 26300 : 36.338951110839844\n",
            "Loss at step 26400 : 45.07676696777344\n",
            "Loss at step 26500 : 32.73331069946289\n",
            "Loss at step 26600 : 39.73375701904297\n",
            "Loss at step 26700 : 51.39931106567383\n",
            "Loss at step 26800 : 34.512386322021484\n",
            "Loss at step 26900 : 40.02564239501953\n",
            "Loss at step 27000 : 31.355066299438477\n",
            "Loss at step 27100 : 34.86002731323242\n",
            "Loss at step 27200 : 35.29436111450195\n",
            "Loss at step 27300 : 39.71204376220703\n",
            "Loss at step 27400 : 37.0517692565918\n",
            "Loss at step 27500 : 46.444969177246094\n",
            "Loss at step 27600 : 34.58865737915039\n",
            "Loss at step 27700 : 48.9840202331543\n",
            "Loss at step 27800 : 51.789791107177734\n",
            "Loss at step 27900 : 31.27545738220215\n",
            "Loss at step 28000 : 30.938596725463867\n",
            "Loss at step 28100 : 39.696075439453125\n",
            "Loss at step 28200 : 37.253536224365234\n",
            "Loss at step 28300 : 33.4073486328125\n",
            "Loss at step 28400 : 36.337615966796875\n",
            "Loss at step 28500 : 34.08103561401367\n",
            "Loss at step 28600 : 36.909603118896484\n",
            "Loss at step 28700 : 32.45071029663086\n",
            "Loss at step 28800 : 44.554988861083984\n",
            "Loss at step 28900 : 40.74446105957031\n",
            "Loss at step 29000 : 37.03614807128906\n",
            "Loss at step 29100 : 38.01374816894531\n",
            "Loss at step 29200 : 32.00356674194336\n",
            "Loss at step 29300 : 46.87224197387695\n",
            "Loss at step 29400 : 36.497650146484375\n",
            "Loss at step 29500 : 40.05902862548828\n",
            "Loss at step 29600 : 37.39954376220703\n",
            "Loss at step 29700 : 43.51325607299805\n",
            "Loss at step 29800 : 33.717472076416016\n",
            "Loss at step 29900 : 38.3913459777832\n",
            "Loss at step 30000 : 32.03181838989258\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 30100 : 41.19561767578125\n",
            "Loss at step 30200 : 49.531333923339844\n",
            "Loss at step 30300 : 38.7226676940918\n",
            "Loss at step 30400 : 39.25225067138672\n",
            "Loss at step 30500 : 32.02058410644531\n",
            "Loss at step 30600 : 42.43352127075195\n",
            "Loss at step 30700 : 41.98810577392578\n",
            "Loss at step 30800 : 44.479469299316406\n",
            "Loss at step 30900 : 37.615962982177734\n",
            "Loss at step 31000 : 39.04286193847656\n",
            "Loss at step 31100 : 40.35834884643555\n",
            "Loss at step 31200 : 39.624046325683594\n",
            "Loss at step 31300 : 32.67381286621094\n",
            "Loss at step 31400 : 40.98237991333008\n",
            "Loss at step 31500 : 35.506927490234375\n",
            "Loss at step 31600 : 31.11366844177246\n",
            "Loss at step 31700 : 41.12891387939453\n",
            "Loss at step 31800 : 43.551570892333984\n",
            "Loss at step 31900 : 36.52516555786133\n",
            "Loss at step 32000 : 30.60576629638672\n",
            "Loss at step 32100 : 35.69757843017578\n",
            "Loss at step 32200 : 33.56729507446289\n",
            "Loss at step 32300 : 54.21099853515625\n",
            "Loss at step 32400 : 39.61094665527344\n",
            "Loss at step 32500 : 45.111328125\n",
            "Loss at step 32600 : 42.10011672973633\n",
            "Loss at step 32700 : 35.11553955078125\n",
            "Loss at step 32800 : 25.5430850982666\n",
            "Loss at step 32900 : 43.00727844238281\n",
            "Loss at step 33000 : 32.06073760986328\n",
            "Loss at step 33100 : 42.90817642211914\n",
            "Loss at step 33200 : 33.477386474609375\n",
            "Loss at step 33300 : 39.68946075439453\n",
            "Loss at step 33400 : 37.868186950683594\n",
            "Loss at step 33500 : 36.37187957763672\n",
            "Loss at step 33600 : 39.882301330566406\n",
            "Loss at step 33700 : 48.88414764404297\n",
            "Loss at step 33800 : 38.456932067871094\n",
            "Loss at step 33900 : 34.55594253540039\n",
            "Loss at step 34000 : 40.88992691040039\n",
            "Loss at step 34100 : 32.345802307128906\n",
            "Loss at step 34200 : 30.847349166870117\n",
            "Loss at step 34300 : 40.13577651977539\n",
            "Loss at step 34400 : 34.7296142578125\n",
            "Loss at step 34500 : 34.24679946899414\n",
            "Loss at step 34600 : 32.776100158691406\n",
            "Loss at step 34700 : 33.48039245605469\n",
            "Loss at step 34800 : 46.60066223144531\n",
            "Loss at step 34900 : 35.50340270996094\n",
            "Loss at step 35000 : 34.0355339050293\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 35100 : 33.84746170043945\n",
            "Loss at step 35200 : 33.29709243774414\n",
            "Loss at step 35300 : 39.38329315185547\n",
            "Loss at step 35400 : 42.58108901977539\n",
            "Loss at step 35500 : 44.583126068115234\n",
            "Loss at step 35600 : 40.41112518310547\n",
            "Loss at step 35700 : 35.264278411865234\n",
            "Loss at step 35800 : 36.85951232910156\n",
            "Loss at step 35900 : 32.69456481933594\n",
            "Loss at step 36000 : 34.665733337402344\n",
            "Loss at step 36100 : 25.46245765686035\n",
            "Loss at step 36200 : 39.593292236328125\n",
            "Loss at step 36300 : 39.70983123779297\n",
            "Loss at step 36400 : 30.24997901916504\n",
            "Loss at step 36500 : 33.02405548095703\n",
            "Loss at step 36600 : 31.381921768188477\n",
            "Loss at step 36700 : 37.17819595336914\n",
            "Loss at step 36800 : 31.32032012939453\n",
            "Loss at step 36900 : 29.9807186126709\n",
            "Loss at step 37000 : 29.278621673583984\n",
            "Loss at step 37100 : 42.91712188720703\n",
            "Loss at step 37200 : 35.27680587768555\n",
            "Loss at step 37300 : 39.724945068359375\n",
            "Loss at step 37400 : 39.971275329589844\n",
            "Loss at step 37500 : 34.24991226196289\n",
            "Loss at step 37600 : 28.98285675048828\n",
            "Loss at step 37700 : 27.422786712646484\n",
            "Loss at step 37800 : 30.965492248535156\n",
            "Loss at step 37900 : 27.933691024780273\n",
            "Loss at step 38000 : 29.450021743774414\n",
            "Loss at step 38100 : 30.5915584564209\n",
            "Loss at step 38200 : 29.728229522705078\n",
            "Loss at step 38300 : 35.379371643066406\n",
            "Loss at step 38400 : 31.427915573120117\n",
            "Loss at step 38500 : 37.825992584228516\n",
            "Loss at step 38600 : 34.29479217529297\n",
            "Loss at step 38700 : 30.386985778808594\n",
            "Loss at step 38800 : 34.693138122558594\n",
            "Loss at step 38900 : 40.296722412109375\n",
            "Loss at step 39000 : 30.117094039916992\n",
            "Loss at step 39100 : 42.48002243041992\n",
            "Loss at step 39200 : 27.449462890625\n",
            "Loss at step 39300 : 30.49930191040039\n",
            "Loss at step 39400 : 32.53582000732422\n",
            "Loss at step 39500 : 31.694116592407227\n",
            "Loss at step 39600 : 37.622047424316406\n",
            "Loss at step 39700 : 31.634044647216797\n",
            "Loss at step 39800 : 42.39749526977539\n",
            "Loss at step 39900 : 31.533573150634766\n",
            "Loss at step 40000 : 31.86427879333496\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 40100 : 36.4708137512207\n",
            "Loss at step 40200 : 24.281597137451172\n",
            "Loss at step 40300 : 30.905248641967773\n",
            "Loss at step 40400 : 38.42837142944336\n",
            "Loss at step 40500 : 37.10636901855469\n",
            "Loss at step 40600 : 35.10258865356445\n",
            "Loss at step 40700 : 29.35893440246582\n",
            "Loss at step 40800 : 33.100711822509766\n",
            "Loss at step 40900 : 33.09854507446289\n",
            "Loss at step 41000 : 33.674476623535156\n",
            "Loss at step 41100 : 29.516277313232422\n",
            "Loss at step 41200 : 32.46105194091797\n",
            "Loss at step 41300 : 40.01707458496094\n",
            "Loss at step 41400 : 29.162004470825195\n",
            "Loss at step 41500 : 29.578275680541992\n",
            "Loss at step 41600 : 36.1214714050293\n",
            "Loss at step 41700 : 41.672054290771484\n",
            "Loss at step 41800 : 36.6522216796875\n",
            "Loss at step 41900 : 26.65672492980957\n",
            "Loss at step 42000 : 30.121135711669922\n",
            "Loss at step 42100 : 27.38175392150879\n",
            "Loss at step 42200 : 26.15852165222168\n",
            "Loss at step 42300 : 32.136695861816406\n",
            "Loss at step 42400 : 33.42399978637695\n",
            "Loss at step 42500 : 37.46096420288086\n",
            "Loss at step 42600 : 43.84522247314453\n",
            "Loss at step 42700 : 33.467811584472656\n",
            "Loss at step 42800 : 31.793588638305664\n",
            "Loss at step 42900 : 25.899019241333008\n",
            "Loss at step 43000 : 29.30913543701172\n",
            "Loss at step 43100 : 27.605792999267578\n",
            "Loss at step 43200 : 30.60887908935547\n",
            "Loss at step 43300 : 41.1307373046875\n",
            "Loss at step 43400 : 33.40007019042969\n",
            "Loss at step 43500 : 37.81806182861328\n",
            "Loss at step 43600 : 25.913217544555664\n",
            "Loss at step 43700 : 38.195491790771484\n",
            "Loss at step 43800 : 25.656692504882812\n",
            "Loss at step 43900 : 31.930103302001953\n",
            "Loss at step 44000 : 28.975330352783203\n",
            "Loss at step 44100 : 28.71331024169922\n",
            "Loss at step 44200 : 34.9995002746582\n",
            "Loss at step 44300 : 29.50127410888672\n",
            "Loss at step 44400 : 32.92306900024414\n",
            "Loss at step 44500 : 38.0229377746582\n",
            "Loss at step 44600 : 31.24966812133789\n",
            "Loss at step 44700 : 27.795900344848633\n",
            "Loss at step 44800 : 32.83809280395508\n",
            "Loss at step 44900 : 23.054485321044922\n",
            "Loss at step 45000 : 28.221906661987305\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 45100 : 29.041711807250977\n",
            "Loss at step 45200 : 26.056468963623047\n",
            "Loss at step 45300 : 37.21786880493164\n",
            "Loss at step 45400 : 28.287996292114258\n",
            "Loss at step 45500 : 41.06317901611328\n",
            "Loss at step 45600 : 22.77281379699707\n",
            "Loss at step 45700 : 36.59114456176758\n",
            "Loss at step 45800 : 30.828901290893555\n",
            "Loss at step 45900 : 23.59158706665039\n",
            "Loss at step 46000 : 22.41688346862793\n",
            "Loss at step 46100 : 33.615447998046875\n",
            "Loss at step 46200 : 31.435672760009766\n",
            "Loss at step 46300 : 25.257373809814453\n",
            "Loss at step 46400 : 33.15764236450195\n",
            "Loss at step 46500 : 26.87399673461914\n",
            "Loss at step 46600 : 25.80307388305664\n",
            "Loss at step 46700 : 27.64293670654297\n",
            "Loss at step 46800 : 18.886478424072266\n",
            "Loss at step 46900 : 24.892786026000977\n",
            "Loss at step 47000 : 22.36506462097168\n",
            "Loss at step 47100 : 33.45896530151367\n",
            "Loss at step 47200 : 36.889244079589844\n",
            "Loss at step 47300 : 34.10109329223633\n",
            "Loss at step 47400 : 30.712717056274414\n",
            "Loss at step 47500 : 32.14761734008789\n",
            "Loss at step 47600 : 34.845314025878906\n",
            "Loss at step 47700 : 28.487546920776367\n",
            "Loss at step 47800 : 33.462745666503906\n",
            "Loss at step 47900 : 34.92183303833008\n",
            "Loss at step 48000 : 33.228336334228516\n",
            "Loss at step 48100 : 29.64950180053711\n",
            "Loss at step 48200 : 32.4109992980957\n",
            "Loss at step 48300 : 33.025047302246094\n",
            "Loss at step 48400 : 28.42259979248047\n",
            "Loss at step 48500 : 23.397550582885742\n",
            "Loss at step 48600 : 21.380882263183594\n",
            "Loss at step 48700 : 40.40229797363281\n",
            "Loss at step 48800 : 31.828231811523438\n",
            "Loss at step 48900 : 26.85211181640625\n",
            "Loss at step 49000 : 35.58774185180664\n",
            "Loss at step 49100 : 29.10462760925293\n",
            "Loss at step 49200 : 25.112390518188477\n",
            "Loss at step 49300 : 20.748132705688477\n",
            "Loss at step 49400 : 35.65489196777344\n",
            "Loss at step 49500 : 32.60585021972656\n",
            "Loss at step 49600 : 27.2075138092041\n",
            "Loss at step 49700 : 34.374881744384766\n",
            "Loss at step 49800 : 28.58526039123535\n",
            "Loss at step 49900 : 36.582489013671875\n",
            "Loss at step 50000 : 22.8654842376709\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 50100 : 34.54478454589844\n",
            "Loss at step 50200 : 44.76588439941406\n",
            "Loss at step 50300 : 28.095914840698242\n",
            "Loss at step 50400 : 24.70171546936035\n",
            "Loss at step 50500 : 27.115175247192383\n",
            "Loss at step 50600 : 22.457321166992188\n",
            "Loss at step 50700 : 25.340078353881836\n",
            "Loss at step 50800 : 29.2394962310791\n",
            "Loss at step 50900 : 25.090898513793945\n",
            "Loss at step 51000 : 24.19696807861328\n",
            "Loss at step 51100 : 44.95200729370117\n",
            "Loss at step 51200 : 25.833240509033203\n",
            "Loss at step 51300 : 29.434288024902344\n",
            "Loss at step 51400 : 28.09103775024414\n",
            "Loss at step 51500 : 24.039546966552734\n",
            "Loss at step 51600 : 26.834482192993164\n",
            "Loss at step 51700 : 27.72715950012207\n",
            "Loss at step 51800 : 18.294879913330078\n",
            "Loss at step 51900 : 38.483150482177734\n",
            "Loss at step 52000 : 28.245607376098633\n",
            "Loss at step 52100 : 30.37245750427246\n",
            "Loss at step 52200 : 26.666854858398438\n",
            "Loss at step 52300 : 25.396902084350586\n",
            "Loss at step 52400 : 32.11751174926758\n",
            "Loss at step 52500 : 24.18270492553711\n",
            "Loss at step 52600 : 22.209264755249023\n",
            "Loss at step 52700 : 32.20478439331055\n",
            "Loss at step 52800 : 34.04255294799805\n",
            "Loss at step 52900 : 31.216419219970703\n",
            "Loss at step 53000 : 21.79810905456543\n",
            "Loss at step 53100 : 25.32040023803711\n",
            "Loss at step 53200 : 19.947341918945312\n",
            "Loss at step 53300 : 19.172876358032227\n",
            "Loss at step 53400 : 26.098594665527344\n",
            "Loss at step 53500 : 35.78266143798828\n",
            "Loss at step 53600 : 28.04026985168457\n",
            "Loss at step 53700 : 36.28418731689453\n",
            "Loss at step 53800 : 29.881103515625\n",
            "Loss at step 53900 : 24.718311309814453\n",
            "Loss at step 54000 : 30.444982528686523\n",
            "Loss at step 54100 : 26.573204040527344\n",
            "Loss at step 54200 : 28.47446632385254\n",
            "Loss at step 54300 : 30.293514251708984\n",
            "Loss at step 54400 : 27.177688598632812\n",
            "Loss at step 54500 : 26.5689754486084\n",
            "Loss at step 54600 : 27.212844848632812\n",
            "Loss at step 54700 : 27.84896469116211\n",
            "Loss at step 54800 : 30.41088104248047\n",
            "Loss at step 54900 : 25.006572723388672\n",
            "Loss at step 55000 : 17.958431243896484\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 55100 : 23.667932510375977\n",
            "Loss at step 55200 : 30.57229232788086\n",
            "Loss at step 55300 : 22.068588256835938\n",
            "Loss at step 55400 : 29.622859954833984\n",
            "Loss at step 55500 : 22.628862380981445\n",
            "Loss at step 55600 : 25.01832389831543\n",
            "Loss at step 55700 : 37.17023468017578\n",
            "Loss at step 55800 : 18.99683952331543\n",
            "Loss at step 55900 : 32.19160461425781\n",
            "Loss at step 56000 : 29.399213790893555\n",
            "Loss at step 56100 : 33.8786506652832\n",
            "Loss at step 56200 : 43.06252670288086\n",
            "Loss at step 56300 : 26.86818504333496\n",
            "Loss at step 56400 : 22.515039443969727\n",
            "Loss at step 56500 : 36.023799896240234\n",
            "Loss at step 56600 : 22.361658096313477\n",
            "Loss at step 56700 : 22.475078582763672\n",
            "Loss at step 56800 : 28.39617347717285\n",
            "Loss at step 56900 : 27.314348220825195\n",
            "Loss at step 57000 : 19.441253662109375\n",
            "Loss at step 57100 : 34.896209716796875\n",
            "Loss at step 57200 : 24.086872100830078\n",
            "Loss at step 57300 : 22.090999603271484\n",
            "Loss at step 57400 : 27.923425674438477\n",
            "Loss at step 57500 : 21.01217269897461\n",
            "Loss at step 57600 : 27.83309555053711\n",
            "Loss at step 57700 : 29.268165588378906\n",
            "Loss at step 57800 : 21.01118278503418\n",
            "Loss at step 57900 : 24.12999153137207\n",
            "Loss at step 58000 : 22.389524459838867\n",
            "Loss at step 58100 : 32.571632385253906\n",
            "Loss at step 58200 : 26.324905395507812\n",
            "Loss at step 58300 : 23.676679611206055\n",
            "Loss at step 58400 : 17.134981155395508\n",
            "Loss at step 58500 : 23.294902801513672\n",
            "Loss at step 58600 : 20.656648635864258\n",
            "Loss at step 58700 : 26.4497127532959\n",
            "Loss at step 58800 : 23.851184844970703\n",
            "Loss at step 58900 : 29.05767250061035\n",
            "Loss at step 59000 : 20.836910247802734\n",
            "Loss at step 59100 : 29.99524688720703\n",
            "Loss at step 59200 : 27.76523780822754\n",
            "Loss at step 59300 : 23.27978515625\n",
            "Loss at step 59400 : 26.092439651489258\n",
            "Loss at step 59500 : 25.255529403686523\n",
            "Loss at step 59600 : 29.27691078186035\n",
            "Loss at step 59700 : 23.355205535888672\n",
            "Loss at step 59800 : 32.60604476928711\n",
            "Loss at step 59900 : 19.530508041381836\n",
            "Loss at step 60000 : 22.769344329833984\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 60100 : 21.52983856201172\n",
            "Loss at step 60200 : 28.47688102722168\n",
            "Loss at step 60300 : 21.426191329956055\n",
            "Loss at step 60400 : 29.53127098083496\n",
            "Loss at step 60500 : 15.181500434875488\n",
            "Loss at step 60600 : 23.855113983154297\n",
            "Loss at step 60700 : 21.48008155822754\n",
            "Loss at step 60800 : 20.100170135498047\n",
            "Loss at step 60900 : 32.86811065673828\n",
            "Loss at step 61000 : 23.725196838378906\n",
            "Loss at step 61100 : 25.696670532226562\n",
            "Loss at step 61200 : 25.7382869720459\n",
            "Loss at step 61300 : 23.296058654785156\n",
            "Loss at step 61400 : 24.69291877746582\n",
            "Loss at step 61500 : 22.263839721679688\n",
            "Loss at step 61600 : 24.109651565551758\n",
            "Loss at step 61700 : 30.036184310913086\n",
            "Loss at step 61800 : 30.81627082824707\n",
            "Loss at step 61900 : 24.00186538696289\n",
            "Loss at step 62000 : 34.89121627807617\n",
            "Loss at step 62100 : 22.965065002441406\n",
            "Loss at step 62200 : 28.91128921508789\n",
            "Loss at step 62300 : 23.120826721191406\n",
            "Loss at step 62400 : 23.3618106842041\n",
            "Loss at step 62500 : 24.162519454956055\n",
            "Loss at step 62600 : 23.13211441040039\n",
            "Loss at step 62700 : 20.048398971557617\n",
            "Loss at step 62800 : 21.310325622558594\n",
            "Loss at step 62900 : 22.31055450439453\n",
            "Loss at step 63000 : 23.446609497070312\n",
            "Loss at step 63100 : 25.20214080810547\n",
            "Loss at step 63200 : 24.539234161376953\n",
            "Loss at step 63300 : 20.29357147216797\n",
            "Loss at step 63400 : 23.610309600830078\n",
            "Loss at step 63500 : 35.483177185058594\n",
            "Loss at step 63600 : 24.074464797973633\n",
            "Loss at step 63700 : 19.878454208374023\n",
            "Loss at step 63800 : 17.403932571411133\n",
            "Loss at step 63900 : 21.60496711730957\n",
            "Loss at step 64000 : 30.665470123291016\n",
            "Loss at step 64100 : 26.589921951293945\n",
            "Loss at step 64200 : 28.89691162109375\n",
            "Loss at step 64300 : 27.52344512939453\n",
            "Loss at step 64400 : 26.718002319335938\n",
            "Loss at step 64500 : 27.042022705078125\n",
            "Loss at step 64600 : 24.238567352294922\n",
            "Loss at step 64700 : 19.971559524536133\n",
            "Loss at step 64800 : 18.912643432617188\n",
            "Loss at step 64900 : 31.24793815612793\n",
            "Loss at step 65000 : 23.2279109954834\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 65100 : 29.143171310424805\n",
            "Loss at step 65200 : 21.970117568969727\n",
            "Loss at step 65300 : 24.421640396118164\n",
            "Loss at step 65400 : 26.091650009155273\n",
            "Loss at step 65500 : 27.650915145874023\n",
            "Loss at step 65600 : 30.143272399902344\n",
            "Loss at step 65700 : 28.0961971282959\n",
            "Loss at step 65800 : 30.431142807006836\n",
            "Loss at step 65900 : 18.083242416381836\n",
            "Loss at step 66000 : 16.61749839782715\n",
            "Loss at step 66100 : 29.603219985961914\n",
            "Loss at step 66200 : 29.24492835998535\n",
            "Loss at step 66300 : 23.72446632385254\n",
            "Loss at step 66400 : 19.382877349853516\n",
            "Loss at step 66500 : 32.1853141784668\n",
            "Loss at step 66600 : 19.881004333496094\n",
            "Loss at step 66700 : 18.732826232910156\n",
            "Loss at step 66800 : 21.63746452331543\n",
            "Loss at step 66900 : 24.40240478515625\n",
            "Loss at step 67000 : 29.752182006835938\n",
            "Loss at step 67100 : 25.636241912841797\n",
            "Loss at step 67200 : 23.921009063720703\n",
            "Loss at step 67300 : 25.044071197509766\n",
            "Loss at step 67400 : 20.00975799560547\n",
            "Loss at step 67500 : 18.909387588500977\n",
            "Loss at step 67600 : 23.773948669433594\n",
            "Loss at step 67700 : 23.02950668334961\n",
            "Loss at step 67800 : 19.325275421142578\n",
            "Loss at step 67900 : 22.057615280151367\n",
            "Loss at step 68000 : 24.332849502563477\n",
            "Loss at step 68100 : 23.13844108581543\n",
            "Loss at step 68200 : 27.330224990844727\n",
            "Loss at step 68300 : 24.70613670349121\n",
            "Loss at step 68400 : 31.203643798828125\n",
            "Loss at step 68500 : 25.352725982666016\n",
            "Loss at step 68600 : 25.17894172668457\n",
            "Loss at step 68700 : 22.382789611816406\n",
            "Loss at step 68800 : 17.547956466674805\n",
            "Loss at step 68900 : 28.572202682495117\n",
            "Loss at step 69000 : 20.594749450683594\n",
            "Loss at step 69100 : 25.618349075317383\n",
            "Loss at step 69200 : 37.87195587158203\n",
            "Loss at step 69300 : 21.93989372253418\n",
            "Loss at step 69400 : 23.590856552124023\n",
            "Loss at step 69500 : 33.63711166381836\n",
            "Loss at step 69600 : 17.144493103027344\n",
            "Loss at step 69700 : 29.033061981201172\n",
            "Loss at step 69800 : 20.393339157104492\n",
            "Loss at step 69900 : 22.769062042236328\n",
            "Loss at step 70000 : 19.126794815063477\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 70100 : 24.82883644104004\n",
            "Loss at step 70200 : 19.637527465820312\n",
            "Loss at step 70300 : 22.12368392944336\n",
            "Loss at step 70400 : 27.713855743408203\n",
            "Loss at step 70500 : 17.27639389038086\n",
            "Loss at step 70600 : 24.691225051879883\n",
            "Loss at step 70700 : 17.783084869384766\n",
            "Loss at step 70800 : 22.670175552368164\n",
            "Loss at step 70900 : 24.61505126953125\n",
            "Loss at step 71000 : 19.904399871826172\n",
            "Loss at step 71100 : 24.514034271240234\n",
            "Loss at step 71200 : 16.499088287353516\n",
            "Loss at step 71300 : 23.18828582763672\n",
            "Loss at step 71400 : 22.084699630737305\n",
            "Loss at step 71500 : 32.02170181274414\n",
            "Loss at step 71600 : 19.137746810913086\n",
            "Loss at step 71700 : 24.32147789001465\n",
            "Loss at step 71800 : 30.284656524658203\n",
            "Loss at step 71900 : 26.687726974487305\n",
            "Loss at step 72000 : 27.6240177154541\n",
            "Loss at step 72100 : 17.087467193603516\n",
            "Loss at step 72200 : 31.01356315612793\n",
            "Loss at step 72300 : 21.920373916625977\n",
            "Loss at step 72400 : 27.266456604003906\n",
            "Loss at step 72500 : 23.478689193725586\n",
            "Loss at step 72600 : 18.38815689086914\n",
            "Loss at step 72700 : 24.152711868286133\n",
            "Loss at step 72800 : 22.200523376464844\n",
            "Loss at step 72900 : 16.579011917114258\n",
            "Loss at step 73000 : 20.748266220092773\n",
            "Loss at step 73100 : 24.86764907836914\n",
            "Loss at step 73200 : 19.58399772644043\n",
            "Loss at step 73300 : 17.634490966796875\n",
            "Loss at step 73400 : 22.575225830078125\n",
            "Loss at step 73500 : 21.352128982543945\n",
            "Loss at step 73600 : 23.355627059936523\n",
            "Loss at step 73700 : 22.615196228027344\n",
            "Loss at step 73800 : 18.999496459960938\n",
            "Loss at step 73900 : 20.91034507751465\n",
            "Loss at step 74000 : 19.528921127319336\n",
            "Loss at step 74100 : 22.696413040161133\n",
            "Loss at step 74200 : 17.613468170166016\n",
            "Loss at step 74300 : 24.846418380737305\n",
            "Loss at step 74400 : 22.008115768432617\n",
            "Loss at step 74500 : 18.734407424926758\n",
            "Loss at step 74600 : 18.10033416748047\n",
            "Loss at step 74700 : 20.541667938232422\n",
            "Loss at step 74800 : 22.36878776550293\n",
            "Loss at step 74900 : 18.21300506591797\n",
            "Loss at step 75000 : 23.39215660095215\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 75100 : 20.114595413208008\n",
            "Loss at step 75200 : 22.504343032836914\n",
            "Loss at step 75300 : 18.973438262939453\n",
            "Loss at step 75400 : 22.18520164489746\n",
            "Loss at step 75500 : 19.899700164794922\n",
            "Loss at step 75600 : 27.732215881347656\n",
            "Loss at step 75700 : 25.61037826538086\n",
            "Loss at step 75800 : 23.047531127929688\n",
            "Loss at step 75900 : 32.090232849121094\n",
            "Loss at step 76000 : 27.158544540405273\n",
            "Loss at step 76100 : 24.0978946685791\n",
            "Loss at step 76200 : 19.27219581604004\n",
            "Loss at step 76300 : 15.129231452941895\n",
            "Loss at step 76400 : 24.763044357299805\n",
            "Loss at step 76500 : 18.14628791809082\n",
            "Loss at step 76600 : 20.477388381958008\n",
            "Loss at step 76700 : 25.847454071044922\n",
            "Loss at step 76800 : 22.376840591430664\n",
            "Loss at step 76900 : 19.107688903808594\n",
            "Loss at step 77000 : 26.13125991821289\n",
            "Loss at step 77100 : 26.89897346496582\n",
            "Loss at step 77200 : 21.05813980102539\n",
            "Loss at step 77300 : 23.087909698486328\n",
            "Loss at step 77400 : 26.102275848388672\n",
            "Loss at step 77500 : 23.4969539642334\n",
            "Loss at step 77600 : 20.263490676879883\n",
            "Loss at step 77700 : 19.189983367919922\n",
            "Loss at step 77800 : 14.817327499389648\n",
            "Loss at step 77900 : 23.9893856048584\n",
            "Loss at step 78000 : 22.087793350219727\n",
            "Loss at step 78100 : 16.229597091674805\n",
            "Loss at step 78200 : 23.100326538085938\n",
            "Loss at step 78300 : 18.842411041259766\n",
            "Loss at step 78400 : 21.03717613220215\n",
            "Loss at step 78500 : 19.044275283813477\n",
            "Loss at step 78600 : 20.721363067626953\n",
            "Loss at step 78700 : 20.01066780090332\n",
            "Loss at step 78800 : 24.623027801513672\n",
            "Loss at step 78900 : 20.61582374572754\n",
            "Loss at step 79000 : 23.019325256347656\n",
            "Loss at step 79100 : 21.228782653808594\n",
            "Loss at step 79200 : 22.087411880493164\n",
            "Loss at step 79300 : 19.459409713745117\n",
            "Loss at step 79400 : 20.583383560180664\n",
            "Loss at step 79500 : 21.79616355895996\n",
            "Loss at step 79600 : 22.065444946289062\n",
            "Loss at step 79700 : 20.050220489501953\n",
            "Loss at step 79800 : 23.81856918334961\n",
            "Loss at step 79900 : 22.793764114379883\n",
            "Loss at step 80000 : 21.35791778564453\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 80100 : 22.198278427124023\n",
            "Loss at step 80200 : 23.9067440032959\n",
            "Loss at step 80300 : 22.287212371826172\n",
            "Loss at step 80400 : 17.807899475097656\n",
            "Loss at step 80500 : 21.35639190673828\n",
            "Loss at step 80600 : 22.596532821655273\n",
            "Loss at step 80700 : 15.065458297729492\n",
            "Loss at step 80800 : 17.9781436920166\n",
            "Loss at step 80900 : 24.673097610473633\n",
            "Loss at step 81000 : 17.621826171875\n",
            "Loss at step 81100 : 15.338924407958984\n",
            "Loss at step 81200 : 19.68996810913086\n",
            "Loss at step 81300 : 19.93539047241211\n",
            "Loss at step 81400 : 17.98630714416504\n",
            "Loss at step 81500 : 22.038684844970703\n",
            "Loss at step 81600 : 16.82351303100586\n",
            "Loss at step 81700 : 22.739707946777344\n",
            "Loss at step 81800 : 18.42171859741211\n",
            "Loss at step 81900 : 21.176366806030273\n",
            "Loss at step 82000 : 23.257753372192383\n",
            "Loss at step 82100 : 21.057641983032227\n",
            "Loss at step 82200 : 18.8641414642334\n",
            "Loss at step 82300 : 20.284202575683594\n",
            "Loss at step 82400 : 21.23854637145996\n",
            "Loss at step 82500 : 22.487390518188477\n",
            "Loss at step 82600 : 17.963115692138672\n",
            "Loss at step 82700 : 14.714475631713867\n",
            "Loss at step 82800 : 18.016178131103516\n",
            "Loss at step 82900 : 21.40314292907715\n",
            "Loss at step 83000 : 21.788509368896484\n",
            "Loss at step 83100 : 21.730117797851562\n",
            "Loss at step 83200 : 17.28002166748047\n",
            "Loss at step 83300 : 15.358901977539062\n",
            "Loss at step 83400 : 20.38813018798828\n",
            "Loss at step 83500 : 30.409452438354492\n",
            "Loss at step 83600 : 23.148557662963867\n",
            "Loss at step 83700 : 20.718544006347656\n",
            "Loss at step 83800 : 21.952674865722656\n",
            "Loss at step 83900 : 19.289112091064453\n",
            "Loss at step 84000 : 18.70867156982422\n",
            "Loss at step 84100 : 15.969141006469727\n",
            "Loss at step 84200 : 19.653011322021484\n",
            "Loss at step 84300 : 19.788530349731445\n",
            "Loss at step 84400 : 18.85429573059082\n",
            "Loss at step 84500 : 22.743648529052734\n",
            "Loss at step 84600 : 23.843812942504883\n",
            "Loss at step 84700 : 20.594270706176758\n",
            "Loss at step 84800 : 22.817264556884766\n",
            "Loss at step 84900 : 16.724300384521484\n",
            "Loss at step 85000 : 18.373384475708008\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 85100 : 17.164796829223633\n",
            "Loss at step 85200 : 17.22208595275879\n",
            "Loss at step 85300 : 21.0792179107666\n",
            "Loss at step 85400 : 20.667409896850586\n",
            "Loss at step 85500 : 15.497724533081055\n",
            "Loss at step 85600 : 20.772363662719727\n",
            "Loss at step 85700 : 23.515270233154297\n",
            "Loss at step 85800 : 21.792240142822266\n",
            "Loss at step 85900 : 21.19461441040039\n",
            "Loss at step 86000 : 28.313264846801758\n",
            "Loss at step 86100 : 18.099334716796875\n",
            "Loss at step 86200 : 24.12033462524414\n",
            "Loss at step 86300 : 29.071929931640625\n",
            "Loss at step 86400 : 20.113065719604492\n",
            "Loss at step 86500 : 18.83887481689453\n",
            "Loss at step 86600 : 17.422536849975586\n",
            "Loss at step 86700 : 22.433576583862305\n",
            "Loss at step 86800 : 18.975305557250977\n",
            "Loss at step 86900 : 15.574042320251465\n",
            "Loss at step 87000 : 21.07501220703125\n",
            "Loss at step 87100 : 19.467552185058594\n",
            "Loss at step 87200 : 19.159137725830078\n",
            "Loss at step 87300 : 19.503358840942383\n",
            "Loss at step 87400 : 21.745359420776367\n",
            "Loss at step 87500 : 23.24765968322754\n",
            "Loss at step 87600 : 20.37811851501465\n",
            "Loss at step 87700 : 16.98743438720703\n",
            "Loss at step 87800 : 17.78475570678711\n",
            "Loss at step 87900 : 19.188888549804688\n",
            "Loss at step 88000 : 15.398111343383789\n",
            "Loss at step 88100 : 14.932060241699219\n",
            "Loss at step 88200 : 17.357751846313477\n",
            "Loss at step 88300 : 16.384124755859375\n",
            "Loss at step 88400 : 15.178266525268555\n",
            "Loss at step 88500 : 21.710113525390625\n",
            "Loss at step 88600 : 15.539648056030273\n",
            "Loss at step 88700 : 17.222089767456055\n",
            "Loss at step 88800 : 17.911746978759766\n",
            "Loss at step 88900 : 24.424592971801758\n",
            "Loss at step 89000 : 22.87623405456543\n",
            "Loss at step 89100 : 16.849943161010742\n",
            "Loss at step 89200 : 21.828073501586914\n",
            "Loss at step 89300 : 27.273786544799805\n",
            "Loss at step 89400 : 29.76439094543457\n",
            "Loss at step 89500 : 19.592592239379883\n",
            "Loss at step 89600 : 23.309558868408203\n",
            "Loss at step 89700 : 16.673328399658203\n",
            "Loss at step 89800 : 21.38010025024414\n",
            "Loss at step 89900 : 19.79482650756836\n",
            "Loss at step 90000 : 15.164115905761719\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 90100 : 16.59779930114746\n",
            "Loss at step 90200 : 17.648681640625\n",
            "Loss at step 90300 : 19.43106460571289\n",
            "Loss at step 90400 : 20.100059509277344\n",
            "Loss at step 90500 : 20.111452102661133\n",
            "Loss at step 90600 : 17.77542495727539\n",
            "Loss at step 90700 : 24.52183723449707\n",
            "Loss at step 90800 : 18.864355087280273\n",
            "Loss at step 90900 : 20.604400634765625\n",
            "Loss at step 91000 : 18.02067756652832\n",
            "Loss at step 91100 : 22.985713958740234\n",
            "Loss at step 91200 : 18.260244369506836\n",
            "Loss at step 91300 : 22.80756378173828\n",
            "Loss at step 91400 : 18.40980339050293\n",
            "Loss at step 91500 : 21.51470375061035\n",
            "Loss at step 91600 : 20.15156364440918\n",
            "Loss at step 91700 : 22.182003021240234\n",
            "Loss at step 91800 : 14.955290794372559\n",
            "Loss at step 91900 : 18.190839767456055\n",
            "Loss at step 92000 : 22.420841217041016\n",
            "Loss at step 92100 : 19.685405731201172\n",
            "Loss at step 92200 : 21.30430793762207\n",
            "Loss at step 92300 : 20.7233829498291\n",
            "Loss at step 92400 : 22.6458683013916\n",
            "Loss at step 92500 : 16.639984130859375\n",
            "Loss at step 92600 : 17.30860137939453\n",
            "Loss at step 92700 : 22.766817092895508\n",
            "Loss at step 92800 : 19.076915740966797\n",
            "Loss at step 92900 : 20.595186233520508\n",
            "Loss at step 93000 : 13.790460586547852\n",
            "Loss at step 93100 : 20.317951202392578\n",
            "Loss at step 93200 : 15.298818588256836\n",
            "Loss at step 93300 : 19.72366714477539\n",
            "Loss at step 93400 : 29.926633834838867\n",
            "Loss at step 93500 : 18.712675094604492\n",
            "Loss at step 93600 : 22.468982696533203\n",
            "Loss at step 93700 : 24.187824249267578\n",
            "Loss at step 93800 : 14.965658187866211\n",
            "Loss at step 93900 : 16.141075134277344\n",
            "Loss at step 94000 : 18.458715438842773\n",
            "Loss at step 94100 : 20.420658111572266\n",
            "Loss at step 94200 : 18.763906478881836\n",
            "Loss at step 94300 : 25.14307403564453\n",
            "Loss at step 94400 : 19.083099365234375\n",
            "Loss at step 94500 : 21.249849319458008\n",
            "Loss at step 94600 : 22.761764526367188\n",
            "Loss at step 94700 : 19.68931007385254\n",
            "Loss at step 94800 : 24.08660888671875\n",
            "Loss at step 94900 : 19.939834594726562\n",
            "Loss at step 95000 : 13.701701164245605\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n",
            "Loss at step 95100 : 20.080533981323242\n",
            "Loss at step 95200 : 22.715261459350586\n",
            "Loss at step 95300 : 20.48857879638672\n",
            "Loss at step 95400 : 17.617631912231445\n",
            "Loss at step 95500 : 12.501008033752441\n",
            "Loss at step 95600 : 23.883405685424805\n",
            "Loss at step 95700 : 19.4615535736084\n",
            "Loss at step 95800 : 23.62138557434082\n",
            "Loss at step 95900 : 15.410588264465332\n",
            "Loss at step 96000 : 19.26140785217285\n",
            "Loss at step 96100 : 21.295406341552734\n",
            "Loss at step 96200 : 18.68506622314453\n",
            "Loss at step 96300 : 18.54323387145996\n",
            "Loss at step 96400 : 18.861196517944336\n",
            "Loss at step 96500 : 23.7338924407959\n",
            "Loss at step 96600 : 19.826955795288086\n",
            "Loss at step 96700 : 21.167573928833008\n",
            "Loss at step 96800 : 16.728883743286133\n",
            "Loss at step 96900 : 20.375537872314453\n",
            "Loss at step 97000 : 18.174985885620117\n",
            "Loss at step 97100 : 18.315547943115234\n",
            "Loss at step 97200 : 17.05010223388672\n",
            "Loss at step 97300 : 22.170289993286133\n",
            "Loss at step 97400 : 22.01150131225586\n",
            "Loss at step 97500 : 18.12810516357422\n",
            "Loss at step 97600 : 19.17776870727539\n",
            "Loss at step 97700 : 16.690372467041016\n",
            "Loss at step 97800 : 17.47955894470215\n",
            "Loss at step 97900 : 13.080275535583496\n",
            "Loss at step 98000 : 17.518230438232422\n",
            "Loss at step 98100 : 17.609949111938477\n",
            "Loss at step 98200 : 19.615934371948242\n",
            "Loss at step 98300 : 17.076351165771484\n",
            "Loss at step 98400 : 13.631414413452148\n",
            "Loss at step 98500 : 23.30072021484375\n",
            "Loss at step 98600 : 17.994464874267578\n",
            "Loss at step 98700 : 20.066621780395508\n",
            "Loss at step 98800 : 14.768356323242188\n",
            "Loss at step 98900 : 18.755115509033203\n",
            "Loss at step 99000 : 15.765067100524902\n",
            "Loss at step 99100 : 19.620677947998047\n",
            "Loss at step 99200 : 16.66053009033203\n",
            "Loss at step 99300 : 18.167449951171875\n",
            "Loss at step 99400 : 16.151126861572266\n",
            "Loss at step 99500 : 18.328933715820312\n",
            "Loss at step 99600 : 15.336727142333984\n",
            "Loss at step 99700 : 23.833908081054688\n",
            "Loss at step 99800 : 16.719911575317383\n",
            "Loss at step 99900 : 13.280134201049805\n",
            "Loss at step 100000 : 19.555524826049805\n",
            "Nearest to love: korea, curse, guys, cheek, respect,\n",
            "Nearest to hate: delightfully, significance, depiction, heaven, bear,\n",
            "Nearest to happy: altogether, exuberance, heavyhandedness, incompetent, disney,\n",
            "Nearest to sad: fascinate, barrel, blair, fearless, amy,\n",
            "Nearest to man: una, filmmakers, night, lick, frighten,\n",
            "Nearest to woman: symmetry, common, gentility, basketball, interview,\n",
            "Model saved in file: /content/temp/doc2vec_movie_embeddings.ckpt\n"
          ]
        }
      ]
    }
  ]
}